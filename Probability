#1) Look at Google "What are the chances of..." to get inspriations of problems to be solved.
#2) statistical inference builds upon probability theory.
#3) Introduce important concepts such as random variables, independence,
#Monte Carlo simulation, expected values, standard errors, margin of errors,
#and the central limit theorem.

###########################################
# 1.1 Introduction to Discrete Probabilty #
###########################################

#discrete probability
#create an urn with 2 red and 3 blue beads
beads <- rep(c("red", "blue"), times = c(2,3)) 

#sample 1 bead at random. Bear in mind that this function means beads will not be replaced after each turn.
sample(beads, 1) 

#Monte Carlo Simulations is needed because we can't run simulations forever.
#On below example, if B is large enough, we will get very close to 40:60 probability

B <- 10000    # number of times to draw 1 bead
events <- replicate(B, sample(beads, 1))    # user replicate function to repeat stimulation. Draw 1 bead, B times
tab <- table(events)    # make a table of outcome counts
tab    # view count table
prop.table(tab)    # view table of outcome proportions

#user sample function, but with replacement, so we put back the bead in the urn after each turn 
events <- sample(beads, B, replace=True) 
prop.table(table(events))

#set the seeds
set.seed(1986) #try use different number every time e.g. year minus month minus day

set.seed(1)
set.seed(1, sample.kind="Rounding")    # will make R 3.6 generate a seed as in R 3.5

#Use mean function to find the probability of a logical vector
#Example... 
beads <- rep(c("red", "blue"), times = c(2,3))
beads
[1] "red" "red" "blue" "blue" "blue"

mean(beads == "blue")
[1] 0.6

#Explanation...
#R evaluates the logical statement beads == "blue", which generates the vector
#FALSE FALSE TRUE TRUE TRUE
#R coerces the logical values to numeric values
# 0 0 1 1 1

#probability distribution is simple with discrete probability 
#e.g. polling of has 44% Democrat, 44% Republican, 10% undecided, and 2% green
#continuous probability will be more complex

#independence - two events are independent when the outcome of one doesn't affect the other
#e.g. tossing a coin, no matter what result you have previously, each time is an equal chance of head/tail

#conditional probability - the probabilities change once you see the other outcomes
#e.g. blackjack after seeing 1 King, the chance of getting a King changes
# Probability of Card2 is a King GIVEN THAT Card1 is a King?
# Pr(Card 2 is a king ∣ Card 1 is a king)=3/51

# If two events  A  and  B  are independent,  Pr(A∣B)=Pr(A) because it doesn't matter what B is, probability of A doesn't change.

#Multiplication Rule - find the probablity of 2 events. 
#Probability of A multiplie by probability of B given A has already happened.
# Pr(A and B) = Pr(A)Pr(B|A)

#Black jack as example - we want an Ace and then a face card or a 10 to hit 21 points.
# Probability of Ace = 1/13
# Probability of face card or 10 = 16/51 
#[16 because...10,J,Q,K x 4] [51 because...one card, Ace is alredy shown]
# 1/13 * 16/51 = 0.02

#Multiplication rule - use induction to extend beyond 2 events
# Pr(A and B and C) = Pr(A) * Pr(B | A) * Pr(C | A and B)

#Multiplication rule - simpler with independent events
# Pr(A and B and C) = Pr(A) * Pr(B) * Pr(C)

#Hence, in real cases, assumption of independence would cause huge differencein probability

Equations:
The multiplication rule for independent events is:
Pr(A and B and C)=Pr(A)×Pr(B)×Pr(C) 

The multiplication rule for dependent events considers the conditional probability of both events occurring:
Pr(A and B)=Pr(A)×Pr(B∣A) 

We can expand the multiplication rule for dependent events to more than 2 events:
Pr(A and B and C)=Pr(A)×Pr(B∣A)×Pr(C∣A and B)

#####################
# DataCamp exercise #
#####################

# Exercise 1. Probability of cyan - generalized
cyan <- 3
magenta <- 5
yellow <- 7
# Assign a variable `p` as the probability of choosing a cyan ball from the box
p <- cyan/(cyan+magenta+yellow)
# Print the variable `p` to the console
p

# Exercise 2. Probability of not cyan - generalized
# `p` is defined as the probability of choosing a cyan ball from a box containing: 3 cyan balls, 5 magenta balls, and 7 yellow balls.
# Using variable `p`, calculate the probability of choosing any ball that is not cyan from the box
1 - p

# Exercise 3. Sampling without replacement - generalized
cyan <- 3
magenta <- 5
yellow <- 7
# The variable `p_1` is the probability of choosing a cyan ball from the box on the first draw.
p_1 <- cyan / (cyan + magenta + yellow)
# Assign a variable `p_2` as the probability of not choosing a cyan ball on the second draw without replacement.
p_2 <- (magenta + yellow)/((cyan-1) + magenta + yellow)
# Calculate the probability that the first draw is cyan and the second draw is not cyan using `p_1` and `p_2`.
p_2 * p_1

#Exercise 4. Sampling with replacement - generalized
cyan <- 3
magenta <- 5
yellow <- 7
# The variable 'p_1' is the probability of choosing a cyan ball from the box on the first draw.
p_1 <- cyan / (cyan + magenta + yellow)
# Assign a variable 'p_2' as the probability of not choosing a cyan ball on the second draw with replacement.
p_2 <- (magenta + yellow) / (cyan + magenta + yellow)
# Calculate the probability that the first draw is cyan and the second draw is not cyan using `p_1` and `p_2`.
p_1 * p_2


###########################################
# 1.2 Combinations and Permutations       #
###########################################
Textbook link
Here is a link to the textbook section on combinations and permutations.
[https://rafalab.github.io/dsbook/probability.html#combinations-and-permutations]

Key points:
paste joins two strings and inserts a space in between.
expand.grid gives the combinations of 2 vectors or lists.
permutations(n,r) from the gtools package lists the different ways that r items can be selected from a set of n options when order matters.
combinations(n,r) from the gtools package lists the different ways that r items can be selected from a set of n options when order does not matter.
Code: Introducing paste and expand.grid

# joining strings with paste
number <- "Three"
suit <- "Hearts"
paste(number, suit)

# joining vectors element-wise with paste
paste(letters[1:5], as.character(1:5))

# generating combinations of 2 vectors with expand.grid
expand.grid(pants = c("blue", "black"), shirt = c("white", "grey", "plaid"))

Code: Generating a deck of cards

suits <- c("Diamonds", "Clubs", "Hearts", "Spades")
numbers <- c("Ace", "Deuce", "Three", "Four", "Five", "Six", "Seven", "Eight", "Nine", "Ten", "Jack", "Queen", "King")
deck <- expand.grid(number = numbers, suit = suits)
deck <- paste(deck$number, deck$suit)   # paste them together so we can pick each of them as an individual

# probability of drawing a king
kings <- paste("King", suits)
mean(deck %in% kings)     # mean gives us the average of when kings happens, thus the probablity

Code: Permutations and combinations

library(gtools)
permutations(5,2)    # ways to choose 2 numbers in order from 1:5
# Notice that the order matters. So 3, 1 is different than 1, 3, So it appears in our permutations. Also notice that 1, 1; 2, 2; and 3, 3 don't appear, because once we pick a number, it can't appear again.  

all_phone_numbers <- permutations(10, 7, v = 0:9)   # added a vector that only picks up 0-9, isntead of 1-10
n <- nrow(all_phone_numbers)
index <- sample(n, 5)
all_phone_numbers[index,]

permutations(3,2)    # order matters
combinations(3,2)    # order does not matter

Code: Probability of drawing a second king given that one king is drawn

hands <- permutations(52,2, v = deck)   # all possible ways to draw 2 cards from ther deck
first_card <- hands[,1]     # grab first column, first card
second_card <- hands[,2]    # grab second column, second card
sum(first_card %in% kings)  # 204, but what fraction of these 204 have also a kind on second card?
 
sum(first_card %in% kings & second_card %in% kings) / sum(first_card %in% kings)  #3 out 3 of 51
# using Sum is equivalent to Mean, will get the same answer
# this is R version of the mutliplication rule    Pr(B|A) = Pr(A&B)/Pr(A)

Code: Probability of a natural 21 in blackjack

aces <- paste("Ace", suits)

facecard <- c("King", "Queen", "Jack", "Ten")
facecard <- expand.grid(number = facecard, suit = suits)
facecard <- paste(facecard$number, facecard$suit)

hands <- combinations(52, 2, v=deck) # all possible hands

# probability of a natural 21 given that the ace is listed first in `combinations`
mean(hands[,1] %in% aces & hands[,2] %in% facecard)

# probability of a natural 21 checking for both ace first and ace second
mean((hands[,1] %in% aces & hands[,2] %in% facecard)|(hands[,2] %in% aces & hands[,1] %in% facecard))
 
# both above get same answer due to the way Combination works, that orders don't matter and counts will not be repeated.

Code: Monte Carlo simulation of natural 21 in blackjack

Note that your exact values will differ because the process is random and the seed is not set.

# code for one hand of blackjack
hand <- sample(deck, 2)
hand

# code for B=10,000 hands of blackjack
B <- 10000
results <- replicate(B, {
  hand <- sample(deck, 2)
  (hand[1] %in% aces & hand[2] %in% facecard) | (hand[2] %in% aces & hand[1] %in% facecard)
})
mean(results)


# The birthday problem
Here is a link to the textbook section on the birthday problem.
[https://rafalab.github.io/dsbook/probability.html#birthday-problem]

Key points

duplicated takes a vector and returns a vector of the same length with TRUE for any elements that have appeared previously in that vector.
We can compute the probability of shared birthdays in a group of people by modeling birthdays as random draws from the numbers 1 through 365. We can then use this sampling model of birthdays to run a Monte Carlo simulation to estimate the probability of shared birthdays.
Code: The birthday problem

# checking for duplicated bdays in one 50 person group
n <- 50
bdays <- sample(1:365, n, replace = TRUE)    # generate n random birthdays, 50 times, with replacement
any(duplicated(bdays))    # check if any birthdays are duplicated

# Monte Carlo simulation with B=10000 replicates
B <- 10000
results <- replicate(B, {    # returns vector of B logical values i.e. repeat simulation B number of times
    bdays <- sample(1:365, n, replace = TRUE)
    any(duplicated(bdays))
})
mean(results)    # calculates proportion of groups with duplicated bdays
 

## sapply
extbook links
The textbook discussion of the basics of sapply can be found in this textbook section.
[https://rafalab.github.io/dsbook/programming-basics.html#vectorization]
The textbook discussion of sapply for the birthday problem can be found within the birthday problem section.
[https://rafalab.github.io/dsbook/probability.html#birthday-problem]
 
Key points:
Some functions automatically apply element-wise to vectors, such as sqrt and *.
However, other functions do not operate element-wise by default. This includes functions we define ourselves.
The function sapply(x, f) allows any other function f to be applied element-wise to the vector x.

The probability of an event happening is 1 minus the probability of that event not happening:
 
          Pr(event)=1−Pr(no event) 
 
We can compute the probability of shared birthdays mathematically:

       Pr(shared birthdays) = 1−Pr(no shared birthdays) = 1−(1 × 364/365 × 363/365 ×...× 365−n+1/365) 
 
Code: Function for calculating birthday problem Monte Carlo simulations for any value of n

Note that the function body of compute_prob is the code that we wrote in the previous video. If we write this code as a function, we can use sapply to apply this function to several values of n.


# function to calculate probability of shared bdays across n people
compute_prob <- function(n, B = 10000) {
    same_day <- replicate(B, {
        bdays <- sample(1:365, n, replace = TRUE)
        any(duplicated(bdays))
    })
    mean(same_day)
}

n <- seq(1, 60)

#we can use for loop, but for loop is rare in R. Operate on vectors in element fashion. For example...
x <- 1:10  # X is now the vector starting at 1 and ending at 10,
sqrt(x)  # we compute the square root of x, it actually computes the square root for each element.
y <- 1:10  # Equally, if we define y to be 1 through 10, 
x*y  # and then multiply x by y, it mutiple each element 1 by 1. So no need for loop.
  
Code: Element-wise operation over vectors and sapply

x <- 1:10
sqrt(x)    # sqrt operates on each element of the vector

y <- 1:10
x*y    # * operates element-wise on both vectors

compute_prob(n)    # does not iterate over the vector n without sapply

x <- 1:10
sapply(x, sqrt)    # this is equivalent to sqrt(x)

prob <- sapply(n, compute_prob)    # element-wise application of compute_prob to n
plot(n, prob)

Computing birthday problem probabilities with sapply
# simpler maths to compute of it NOT happening i.e. probability of who has unique birthday. 
 # So the probability for the 1st person to have unique birthday is 1.
# function for computing exact probability of shared birthdays for any n
exact_prob <- function(n){
    prob_unique <- seq(365, 365-n+1)/365   # vector of fractions for mult. rule. 365-n+1 means the index of your turn minus 1. e.g. Pr(3rd person has unique bday | 1st & 2nd have unique bdays) = 363/365
    1 - prod(prob_unique)    # calculate prob of no shared birthdays and subtract from 1
}

# applying function element-wise to vector of n values
eprob <- sapply(n, exact_prob)

# plotting Monte Carlo results and exact probabilities on same graph
plot(n, prob)    # plot Monte Carlo results
lines(n, eprob, col = "red")    # add line for exact prob
 

## How many Monte Carlo experiments are enough? 
Textbook link:
Here is a link to the matching textbook section.
 [https://rafalab.github.io/dsbook/probability.html#infinity-in-practice]

Key points

The larger the number of Monte Carlo replicates  B , the more accurate the estimate.
Determining the appropriate size for  B  can require advanced statistics.
One practical approach is to try many sizes for  B  and look for sizes that provide stable estimates.
# it means we wopn't know how many times is enough, really. However, we can usetheoretical statstic to see the stability of estimate.
  
Code: Estimating a practical value of B

This code runs Monte Carlo simulations to estimate the probability of shared birthdays using several B values and plots the results. When B is large enough that the estimated probability stays stable, then we have selected a useful value of B. 

B <- 10^seq(1, 5, len = 100)    # defines vector of many B values e.g. 10,20,40,100,etc.
compute_prob <- function(B, n = 22){    # function to run Monte Carlo simulation with each B
    same_day <- replicate(B, {
        bdays <- sample(1:365, n, replace = TRUE)
        any(duplicated(bdays))
    })
    mean(same_day)
}
prob <- sapply(B, compute_prob)    # apply compute_prob to many values of B
plot(log10(B), prob, type = "l")    # plot a line graph of estimates  
# the graph starts to stablise towards right side indicates when we have stabler estimates.
  
 
# datacamp exercise
# Exercise 2. Sampling with replacement
cyan <- 3
magenta <- 5
yellow <- 7

# Assign the variable 'p_yellow' as the probability that a yellow ball is drawn from the box.
p_yellow <- yellow/(cyan+magenta+yellow)
# Using the variable 'p_yellow', calculate the probability of drawing a yellow ball on the sixth draw. Print this value to the console.

p_yellow
# the events are independant. So Pr(yellow|yellow) = Pr(yellow). So Pr(yellow|yellow|yellow|yellow|yellow|yellow|yellow) = Pr(yellow)
  
 
# Exercise 3. Rolling a die

## Assign the variable 'p_no6' as the probability of not seeing a 6 on a single roll.
p_no6 <- 5/6

# Calculate the probability of not seeing a 6 on six rolls using `p_no6`. Print your result to the console: do not assign it to a variable.
(p_no6)^6    # because outcomes are dependent, so times it 6 times
  
  
Exercise 4. Probability the Celtics win a game
Two teams, say the Celtics and the Cavs, are playing a seven game series. The Cavs are a better team and have a 60% chance of winning each game.

What is the probability that the Celtics win at least one game? Remember that the Celtics must win one of the first four games, or the series will be over!
  
# Assign the variable `p_cavs_win4` as the probability that the Cavs will win the first four games of the series.
p_cavs_win4 <- (0.6)^4

# Using the variable `p_cavs_win4`, calculate the probability that the Celtics win at least one game in the first four games of the series.
1 - p_cavs_win4   # because it's the opposite of Cavs wining 4 times in a row


## Exercise 5. Monte Carlo simulation for Celtics winning a game
Create a Monte Carlo simulation to confirm your answer to the previous problem by estimating how frequently the Celtics win at least 1 of 4 games. Use B <- 10000 simulations.
The provided sample code simulates a single series of four random games, simulated_games.
  
# This line of example code simulates four independent random games where the Celtics either lose or win. Copy this example code to use within the `replicate` function.
simulated_games <- sample(c("lose","win"), 4, replace = TRUE, prob = c(0.6, 0.4))

# The variable 'B' specifies the number of times we want the simulation to run. Let's run the Monte Carlo simulation 10,000 times.
B <- 10000

# Use the `set.seed` function to make sure your answer matches the expected result after random sampling.
set.seed(1)

# Create an object called `celtic_wins` that replicates two steps for B iterations: (1) generating a random four-game series `simulated_games` using the example code, then (2) determining whether the simulated series contains at least one win for the Celtics.
celtic_wins <- replicate(B, {    
    simulated_games <- #set.seed(1) 
    sample(c("lose","win"), 4, replace = TRUE, prob = c(0.6, 0.4))
    any(simulated_games == "win")
})

# Calculate the frequency out of B iterations that the Celtics won at least one game. Print your answer to the console.
mean(celtic_wins)
  
 
##########################################
# 1.3 Addition Rule and Monty Hall       #
##########################################
Textbook link
Here is a link to the textbook section on the addition rule.
[https://rafalab.github.io/dsbook/probability.html#addition-rule]
 
Clarification
By "facecard", the professor means a card with a value of 10 (K, Q, J, 10).

Key points:
The addition rule states that the probability of event  A  or event  B  happening is the probability of event  A  plus the probability of event  B  minus the probability of both events  A  and  B  happening together.
     Pr(A or B)=Pr(A)+Pr(B)−Pr(A and B)   # think of venn diagram, the middle overlay area means we've counted them twice
Note that  (A or B)  is equivalent to  (A|B) .
 
Example: The addition rule for a natural 21 in blackjack
 
We apply the addition rule where  A  = drawing an ace then a facecard and  B  = drawing a facecard then an ace. Note that in this case, both events A and B cannot happen at the same time, so  Pr(A and B)=0 .

Pr(ace then facecard) = 4/52 × 16/51 
Pr(facecard then ace) = 16/52 × 4/51 
Pr(ace then facecard | facecard then ace) = (4/52 × 16/51) + (16/52 × 4/51) = 0.0483  

## Monty Hall problem
 Textbook section:
Here is the textbook section on the Monty Hall Problem. [https://rafalab.github.io/dsbook/probability.html#monty-hall-problem]

Error in Monty Hall explanation
At 0:32, the professor incorrectly says that Monty Hall opens one of the two remaining doors only if the contestant did not pick the prize door on the first try. In the actual problem, Monty Hall always opens one of the two remaining doors (never revealing the prize). The Monte Carlo simulation code is correct for the actual problem.

Key points
Monte Carlo simulations can be used to simulate random outcomes, which makes them useful when exploring ambiguous or less intuitive problems like the Monty Hall problem.
In the Monty Hall problem, contestants choose one of three doors that may contain a prize. Then, one of the doors that was not chosen by the contestant and does not contain a prize is revealed. The contestant can then choose whether to stick with the original choice or switch to the remaining unopened door.
Although it may seem intuitively like the contestant has a 1 in 2 chance of winning regardless of whether they stick or switch, Monte Carlo simulations demonstrate that the actual probability of winning is 1 in 3 with the stick strategy and 2 in 3 with the switch strategy.
For more on the Monty Hall problem, you can watch a detailed explanation here or read an explanation here.
[https://www.khanacademy.org/math/precalculus/prob-comb/dependent-events-precalc/v/monty-hall-problem]
[https://en.wikipedia.org/wiki/Monty_Hall_problem]

Code: Monte Carlo simulation of stick strategy
B <- 10000
stick <- replicate(B, {
  doors <- as.character(1:3)
  prize <- sample(c("car","goat","goat"))    # puts prizes in random order
  prize_door <- doors[prize == "car"]    # note which door has prize
  my_pick  <- sample(doors, 1)    # note which door is chosen
  show <- sample(doors[!doors %in% c(my_pick, prize_door)],1)    # open door with no prize that isn't chosen
  stick <- my_pick    # stick with original door
  stick == prize_door    # test whether the original door has the prize
})
mean(stick)    # probability of choosing prize door when sticking
Code: Monte Carlo simulation of switch strategy
switch <- replicate(B, {
  doors <- as.character(1:3)
  prize <- sample(c("car","goat","goat"))    # puts prizes in random order
  prize_door <- doors[prize == "car"]    # note which door has prize
  my_pick  <- sample(doors, 1)    # note which door is chosen first
  show <- sample(doors[!doors %in% c(my_pick, prize_door)], 1)    # open door with no prize that isn't chosen
  switch <- doors[!doors%in%c(my_pick, show)]    # switch to the door that wasn't chosen first or opened
  switch == prize_door    # test whether the switched door has the prize
})
mean(switch)    # probability of choosing prize door when switching

## Data camp exercises
## Exercise 1. The Cavs and the Warriors
Two teams, say the Cavs and the Warriors, are playing a seven game championship series. 
The first to win four games wins the series. The teams are equally good, so they each have a 50-50 chance of winning each game.

If the Cavs lose the first game, what is the probability that they win the series?

Intructions:
Assign the number of remaining games to the variable n.
Assign a variable outcomes as a vector of possible outcomes in a single game, where 0 indicates a loss and 1 indicates a win for the Cavs.
Assign a variable l to a list of all possible outcomes in all remaining games. Use the rep function to create a list of n games, where each game consists of list(outcomes).
Use the expand.grid function to create a data frame containing all the combinations of possible outcomes of the remaining games.
Use the rowSums function to identify which combinations of game outcomes result in the Cavs winning the number of games necessary to win the series.
Use the mean function to calculate the proportion of outcomes that result in the Cavs winning the series and print your answer to the console.

# Assign a variable 'n' as the number of remaining games.
n <- 6

# Assign a variable `outcomes` as a vector of possible game outcomes, where 0 indicates a loss and 1 indicates a win for the Cavs.
outcomes <- c(0,1)

# Assign a variable `l` to a list of all possible outcomes in all remaining games. Use the `rep` function on `list(outcomes)` to create list of length `n`.
l <- rep(list(outcomes), n)

# Create a data frame named 'possibilities' that contains all combinations of possible outcomes for the remaining games.
possibilities <- expand.grid(l)

# Create a vector named 'results' that indicates whether each row in the data frame 'possibilities' contains enough wins for the Cavs to win the series.
results <- rowSums(possibilities) >=4

# Calculate the proportion of 'results' in which the Cavs win the series. Print the outcome to the console.
mean(results)

                                                         
## Exercise 2. The Cavs and the Warriors - Monte Carlo
Confirm the results of the previous question with a Monte Carlo simulation to estimate the probability of the Cavs winning the series after losing the first game.

# The variable `B` specifies the number of times we want the simulation to run. Let's run the Monte Carlo simulation 10,000 times.
B <- 10000

# Use the `set.seed` function to make sure your answer matches the expected result after random sampling.
set.seed(1)

# Create an object called `results` that replicates for `B` iterations a simulated series and determines whether that series contains at least four wins for the Cavs.
results <- replicate(B, {
  Cavs <- sample(c(0,1), 6, replace=TRUE)
  sum(Cavs)
})

# Calculate the frequency out of `B` iterations that the Cavs won at least four games in the remainder of the series. Print your answer to the console.
mean(results >= 4)


## Exercise 3. A and B play a series - part 1
Two teams, A and B, are playing a seven series game series. Team A is better than team B and has a p>0.5 chance of winning each game.

Use the function sapply to compute the probability, call it Pr of winning for p <- seq(0.5, 0.95, 0.025).

# Let's assign the variable 'p' as the vector of probabilities that team A will win.
p <- seq(0.5, 0.95, 0.025)

# Given a value 'p', the probability of winning the series for the underdog team B can be computed with the following function based on a Monte Carlo simulation:
prob_win <- function(p){
  B <- 10000
  result <- replicate(B, {
    b_win <- sample(c(1,0), 7, replace = TRUE, prob = c(1-p, p))
    sum(b_win)>=4
    })
  mean(result)
}

# Apply the 'prob_win' function across the vector of probabilities that team A will win to determine the probability that team B will win. Call this object 'Pr'.
Pr <- sapply(p, prob_win)

# Plot the probability 'p' on the x-axis and 'Pr' on the y-axis.
plot(p, Pr)
Then plot the result plot(p, Pr).


## Exercise 4. A and B play a series - part 2
Repeat the previous exercise, but now keep the probability that team A wins fixed at p <- 0.75 and compute the probability for different series lengths. 
For example, wins in best of 1 game, 3 games, 5 games, and so on through a series that lasts 25 games.

# Given a value 'p', the probability of winning the series for the underdog team B can be computed with the following function based on a Monte Carlo simulation:
prob_win <- function(N, p=0.75){
      B <- 10000
      result <- replicate(B, {
        b_win <- sample(c(1,0), N, replace = TRUE, prob = c(1-p, p))
        sum(b_win)>=(N+1)/2
        })
      mean(result)
    }

# Assign the variable 'N' as the vector of series lengths. Use only odd numbers ranging from 1 to 25 games.
N <- seq(1, 25, 2)

# Apply the 'prob_win' function across the vector of series lengths to determine the probability that team B will win. Call this object `Pr`.
Pr <- sapply(N, prob_win)

# Plot the number of games in the series 'N' on the x-axis and 'Pr' on the y-axis.
plot(N, Pr)


###########################################
# Section 2 - Continuous Probability      #
###########################################
Section 2 introduces you to Continuous Probability.

After completing Section 2, you will:

understand the differences between calculating probabilities for discrete and continuous data.
be able to use cumulative distribution functions to assign probabilities to intervals when dealing with continuous data.
be able to use R to generate normally distributed outcomes for use in Monte Carlo simulations.
know some of the useful theoretical continuous distributions in addition to the normal distribution, such as the student-t, chi-squared, exponential, gamma, beta, and beta-binomial distributions.
There is 1 assignment that uses the DataCamp platform for you to practice your coding skills as well as a set of questions on the edX platform at the end of section 3.

This section corresponds to the continuous probability section of the course textbook.
[https://rafalab.github.io/dsbook/probability.html#continuous-probability]

## Continuous probability
Textbook links
Ths video corresponds to the textbook section on continuous probability. [https://rafalab.github.io/dsbook/probability.html#continuous-probability]

The previous discussion of CDF is from the Data Visualization course. Here is the textbook section on the CDF. [https://rafalab.github.io/dsbook/distributions.html#cdf-intro]

Key points
The cumulative distribution function (CDF) is a distribution function for continuous data  x  that reports the proportion of the data below  a  for all values of  a :
          F(a)=Pr(x≤a)          
The CDF is the probability distribution function for continuous variables. For example, to determine the probability that a male student is taller than 70.5 inches given a vector of male heights  x , we can use the CDF:
          Pr(x>70.5)=1−Pr(x≤70.5)=1−F(70.5) 
The probability that an observation is in between two values  a,b  is  F(b)−F(a) .

Code: Cumulative distribution function
Define x as male heights from the dslabs dataset:

library(tidyverse)
library(dslabs)
data(heights)
x <- heights %>% filter(sex=="Male") %>% pull(height)

Given a vector x, we can define a function for computing the CDF of x using:

F <- function(a) mean(x <= a)

1 - F(70)    # probability of male taller than 70 inches


## Therotical Distribution
Textbook link
This video corresponds to the textbook section on the theoretical distribution and the normal approximation.
[https://rafalab.github.io/dsbook/probability.html#theoretical-continuous-distributions]

Key points
pnorm(a, avg, s) gives the value of the cumulative distribution function  F(a)  for the normal distribution defined by average avg and standard deviation s.

We say that a random quantity is normally distributed with average avg and standard deviation s if the approximation pnorm(a, avg, s) holds for all values of a.

If we are willing to use the normal approximation for height, we can estimate the distribution simply from the mean and standard deviation of our values.
If we treat the height data as discrete rather than categorical, we see that the data are not very useful because integer values are more common than expected due to rounding. This is called discretization.
With rounded data, the normal approximation is particularly useful when computing probabilities of intervals of length 1 that include exactly one integer.
Code: Using pnorm to calculate probabilities
Given male heights x:

library(tidyverse)
library(dslabs)
data(heights)
x <- heights %>% filter(sex=="Male") %>% pull(height)

We can estimate the probability that a male is taller than 70.5 inches using:

1 - pnorm(70.5, mean(x), sd(x))

Code: Discretization and the normal approximation
# plot distribution of exact heights in data
plot(prop.table(table(x)), xlab = "a = Height in inches", ylab = "Pr(x = a)")

# probabilities in actual data over length 1 ranges containing an integer
mean(x <= 68.5) - mean(x <= 67.5)
mean(x <= 69.5) - mean(x <= 68.5)
mean(x <= 70.5) - mean(x <= 69.5)

# probabilities in normal approximation match well
pnorm(68.5, mean(x), sd(x)) - pnorm(67.5, mean(x), sd(x))
pnorm(69.5, mean(x), sd(x)) - pnorm(68.5, mean(x), sd(x))
pnorm(70.5, mean(x), sd(x)) - pnorm(69.5, mean(x), sd(x))

# probabilities in actual data over other ranges don't match normal approx as well
mean(x <= 70.9) - mean(x <= 70.1)
pnorm(70.9, mean(x), sd(x)) - pnorm(70.1, mean(x), sd(x))


## Probability density
Textbook link
This video corresponds to the textbook section on probability density.

Key points
The probability of a single value is not defined for a continuous distribution.
The quantity with the most similar interpretation to the probability of a single value is the probability density function  f(x) .
The probability density  f(x)  is defined such that the integral of  f(x)  over a range gives the CDF of that range.
          F(a)=Pr(X≤a)=∫a−∞ f(x)dx 
In R, the probability density function for the normal distribution is given by dnorm. We will see uses of dnorm in the future.
Note that dnorm gives the density function and pnorm gives the distribution function, which is the integral of the density function.

## Monte Carlo Simulations
Textbook link
This video corresponds to the textbook section on Monte Carlo simulations for continuous variables.
[https://rafalab.github.io/dsbook/probability.html#monte-carlo-simulations-for-continuous-variables]

Key points
rnorm(n, avg, s) generates n random numbers from the normal distribution with average avg and standard deviation s.

By generating random numbers from the normal distribution, we can simulate height data with similar properties to our dataset. Here we generate simulated height data using the normal distribution.
Code: Generating normally distributed random numbers for Monte Carlo simulations
# define x as male heights from dslabs data
library(tidyverse)
library(dslabs)
data(heights)
x <- heights %>% filter(sex=="Male") %>% pull(height)

# generate simulated height data using normal distribution - both datasets should have n observations
n <- length(x)
avg <- mean(x)
s <- sd(x)
simulated_heights <- rnorm(n, avg, s)

# plot distribution of simulated_heights
data.frame(simulated_heights = simulated_heights) %>%
    ggplot(aes(simulated_heights)) +
    geom_histogram(color="black", binwidth = 2)

Code: Monte Carlo simulation of probability of tallest person being over 7 feet
B <- 10000
tallest <- replicate(B, {
    simulated_data <- rnorm(800, avg, s)    # generate 800 normally distributed random heights
    max(simulated_data)    # determine the tallest height 
})
mean(tallest >= 7*12)    # proportion of times that tallest person exceeded 7 feet (84 inches)


## Other Continuous Distributions
Textbook link
This video corresponds to the textbook section on other continuous distributions.
[https://rafalab.github.io/dsbook/probability.html#continuous-distributions]

Key points:
You may encounter other continuous distributions (Student t, chi-squared, exponential, gamma, beta, etc.).
R provides functions for density (d), quantile (q), probability distribution (p) and random number generation (r) for many of these distributions.
Each distribution has a matching abbreviation (for example, norm or t) that is paired with the related function abbreviations (d, p, q, r) to create appropriate functions.
For example, use rt to generate random numbers for a Monte Carlo simulation using the Student t distribution.
Code: Plotting the normal distribution with dnorm

Use d to plot the density function of a continuous distribution. Here is the density function for the normal distribution (abbreviation norm):

x <- seq(-4, 4, length.out = 100)
data.frame(x, f = dnorm(x)) %>%
    ggplot(aes(x,f)) +
    geom_line()
    

## Datacamp exercises
# Exercise 1. Distribution of female heights - 1
Assume the distribution of female heights is approximated by a normal distribution with a mean of 64 inches and a standard deviation of 3 inches. 
If we pick a female at random, what is the probability that she is 5 feet or shorter?
# Assign a variable 'female_avg' as the average female height.
female_avg <- 64

# Assign a variable 'female_sd' as the standard deviation for female heights.
female_sd <- 3

# Using variables 'female_avg' and 'female_sd', calculate the probability that a randomly selected female is shorter than 5 feet. Print this value to the console.
pnorm(5*12, female_avg , female_sd)

# Exercise 2. If we pick a female at random, what is the probability that she is 6 feet or taller?
# Using variables 'female_avg' and 'female_sd', calculate the probability that a randomly selected female is 6 feet or taller. Print this value to the console.
1-pnorm(6*12, female_avg, female_sd)

# Exercise 3. If we pick a female at random, what is the probability that she is between 61 and 67 inches?
# Using variables 'female_avg' and 'female_sd', calculate the probability that a randomly selected female is between the desired height range. Print this value to the console.
pnorm(67,female_avg,female_sd)-pnorm(61,female_avg,female_sd)


# Exercise 5. Probability of 1 SD from average
Compute the probability that the height of a randomly chosen female is within 1 SD from the average height.
# Assign a variable 'female_avg' as the average female height.
female_avg <- 64

# Assign a variable 'female_sd' as the standard deviation for female heights.
female_sd <- 3

# To a variable named 'taller', assign the value of a height that is one SD taller than average.
taller <- female_avg + female_sd

# To a variable named 'shorter', assign the value of a height that is one SD shorter than average.
shorter <- female_avg - female_sd

# Calculate the probability that a randomly selected female is between the desired height range. Print this value to the console.
pnorm(taller,female_avg, female_sd) - pnorm(shorter,female_avg, female_sd)


# Exercise 6. Distribution of male heights
Imagine the distribution of male adults is approximately normal with an average of 69 inches and a standard deviation of 3 inches. 
How tall is a male in the 99th percentile?

# Assign a variable 'male_avg' as the average male height.
male_avg <- 69

# Assign a variable 'male_sd' as the standard deviation for male heights.
male_sd <- 3

# Determine the height of a man in the 99th percentile of the distribution.
qnorm(0.99, male_avg, male_sd)


# Exercise 7. Distribution of IQ scores
The distribution of IQ scores is approximately normally distributed. The average is 100 and the standard deviation is 15. 
Suppose you want to know the distribution of the person with the highest IQ in your school district, where 10,000 people are born each year.

Generate 10,000 IQ scores 1,000 times using a Monte Carlo simulation. Make a histogram of the highest IQ scores.

# The variable `B` specifies the number of times we want the simulation to run.
B <- 1000

# Use the `set.seed` function to make sure your answer matches the expected result after random number generation.
set.seed(1)

# Create an object called `highestIQ` that contains the highest IQ score from each random distribution of 10,000 people.
highestIQ <- replicate(B, {
    simulated_data <- rnorm(10000, 100, 15)   
    max(simulated_data)   
})

# Use function hist, to make a histogram of the highest IQ scores.
hist(highestIQ)



###############################################################################
# Section 3: Random Variables, Sampling Models, and the Central Limit Theorem #
###############################################################################
Section 3 introduces you to Random Variables, Sampling Models, and the Central Limit Theorem.
[https://rafalab.github.io/dsbook/random-variables.html]
Section 3 is divided into two parts:
Random Variables and Sampling Models
The Central Limit Theorem.

Textbook link
This video corresponds to the textbook section on random variables. [https://rafalab.github.io/dsbook/random-variables.html#random-variables-1]

Key points
Random variables are numeric outcomes resulting from random processes.
Statistical inference offers a framework for quantifying uncertainty due to randomness.

Code: Modeling a random variable
# define random variable x to be 1 if blue, 0 otherwise
beads <- rep(c("red", "blue"), times = c(2, 3))
x <- ifelse(sample(beads, 1) == "blue", 1, 0)
# demonstrate that the random variable is different every time
ifelse(sample(beads, 1) == "blue", 1, 0)
ifelse(sample(beads, 1) == "blue", 1, 0)
ifelse(sample(beads, 1) == "blue", 1, 0)


## Sampling Models
Textbook link and additional information
This video corresponds to the textbook section on sampling models. [https://rafalab.github.io/dsbook/random-variables.html#sampling-models]

You can read more about the binomial distribution here. [https://en.wikipedia.org/wiki/Binomial_distribution]

Key points
A sampling model models the random behavior of a process as the sampling of draws from an urn.
The probability distribution of a random variable is the probability of the observed value falling in any given interval.
We can define a CDF  F(a)=Pr(S≤a)  to answer questions related to the probability of S being in any interval.
The average of many draws of a random variable is called its expected value.
The standard deviation of many draws of a random variable is called its standard error.
Monte Carlo simulation: Chance of casino losing money on roulette
We build a sampling model for the random variable  S  that represents the casino's total winnings. 

# sampling model 1: define urn, then sample
color <- rep(c("Black", "Red", "Green"), c(18, 18, 2)) # define the urn for the sampling model
n <- 1000
X <- sample(ifelse(color == "Red", -1, 1), n, replace = TRUE)    # 1000 draws from urn, -1 if red, else +1
X[1:10]    # first 10 outcomes

# sampling model 2: define urn inside sample function by noting probabilities
x <- sample(c(-1, 1), n, replace = TRUE, prob = c(9/19, 10/19))    # 1000 independent draws
S <- sum(x)    # total winnings = sum of draws
S

We use the sampling model to run a Monte Carlo simulation and use the results to estimate the probability of the casino losing money.

n <- 1000    # number of roulette players
B <- 10000    # number of Monte Carlo experiments
S <- replicate(B, {
    X <- sample(c(-1,1), n, replace = TRUE, prob = c(9/19, 10/19))    # simulate 1000 roulette spins
    sum(X)    # determine total profit
})

mean(S < 0)    # probability of the casino losing money

We can plot a histogram of the observed values of S as well as the normal density curve based on the mean and standard deviation of S.

library(tidyverse)

s <- seq(min(S), max(S), length = 100)    # sequence of 100 values across range of S
normal_density <- data.frame(s = s, f = dnorm(s, mean(S), sd(S))) # generate normal density for S
data.frame (S = S) %>%    # make data frame of S for histogram
    ggplot(aes(S, ..density..)) +
    geom_histogram(color = "black", binwidth = 10) +
    ylab("Probability") +
    geom_line(data = normal_density, mapping = aes(s, f), color = "blue")


## Distributions versus Probability Distributions
Textbook link
This video corresponds to the textbook section on distributions versus probability distributions.
[https://rafalab.github.io/dsbook/random-variables.html#distributions-versus-probability-distributions]

Key points
A random variable  X  has a probability distribution function  F(a)  that defines  Pr(X≤a)  over all values of  a .
Any list of numbers has a distribution. The probability distribution function of a random variable is defined mathematically and does not depend on a list of numbers.
The results of a Monte Carlo simulation with a large enough number of observations will approximate the probability distribution of  X .
If a random variable is defined as draws from an urn:
The probability distribution function of the random variable is defined as the distribution of the list of values in the urn.
The expected value of the random variable is the average of values in the urn.
The standard error of one draw of the random variable is the standard deviation of the values of the urn.


## Notation for Random Variables
Textbook link
This video corresponds to the textbook section on notation for random variables.
[https://rafalab.github.io/dsbook/random-variables.html#notation-for-random-variables]

Key points
Capital letters denote random variables ( X ) and lowercase letters denote observed values ( x ).
In the notation  Pr(X=x) , we are asking how frequently the random variable  X  is equal to the value  x . For example, if  x=6 , this statement becomes  Pr(X=6) .

For example, big X equeals the number on a die roll, small x represents the actual vaule we see:
Pr(X=x) = 1/6


## Central Limit Theorem
Textbook links
This video corresponds to the textbook sections on expected value and standard error and the Central Limit Theorem.

Key points
The Central Limit Theorem (CLT) says that the distribution of the sum of a random variable is approximated by a normal distribution.

The expected value of a random variable,  E[X]=μ , is the average of the values in the urn. This represents the expectation of one draw. 
The standard error of one draw of a random variable is the standard deviation of the values in the urn.
The expected value of the sum of draws is the number of draws times the expected value of the random variable. 
The standard error of the sum of independent draws of a random variable is the square root of the number of draws times the standard deviation of the urn. 
Equations
These equations apply to the case where there are only two outcomes,  a  and  b  with proportions  p  and  1−p  respectively. The general principles above also apply to random variables with more than two outcomes.

Expected value of a random variable: 
ap+b(1−p) 

Expected value of the sum of n draws of a random variable: 
n*(ap+b(1−p)) 

Standard deviation of an urn with two values: 

∣b–a∣√p(1−p)−−−−−−− 

Standard error of the sum of n draws of a random variable:
√n−− * ∣b–a∣ √p(1−p)−−−−−−−



## Data camp exercise
Exercise 1. American Roulette probabilities
An American roulette wheel has 18 red, 18 black, and 2 green pockets. Each red and black pocket is associated with a number from 1 to 36. 
The two remaining green slots feature "0" and "00". Players place bets on which pocket they think a ball will land in after the wheel is spun. 
Players can bet on a specific number (0, 00, 1-36) or color (red, black, or green).

What are the chances that the ball lands in a green pocket?

# The variables `green`, `black`, and `red` contain the number of pockets for each color
green <- 2
black <- 18
red <- 18

# Assign a variable `p_green` as the probability of the ball landing in a green pocket
p_green <- green/(green+black+red)

# Print the variable `p_green` to the console
p_green


# Exercise 2. American Roulette payout
In American roulette, the payout for winning on green is $17. This means that if you bet $1 and it lands on green, you get $17 as a prize.
Create a model to predict your winnings from betting on green one time.

# Use the `set.seed` function to make sure your answer matches the expected result after random sampling.
set.seed(1)

# The variables 'green', 'black', and 'red' contain the number of pockets for each color
green <- 2
black <- 18
red <- 18

# Assign a variable `p_green` as the probability of the ball landing in a green pocket
p_green <- green / (green+black+red)

# Assign a variable `p_not_green` as the probability of the ball not landing in a green pocket
p_not_green <- 1-p_green

# Create a model to predict the random variable `X`, your winnings from betting on green. Sample one time.
n <- 1
X <- sample(c(-1, 17), n, replace = TRUE, prob = c(18/19, 1/19)) 

# Print the value of `X` to the console
X




